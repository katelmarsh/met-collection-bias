# OneRule

## Exploratory Analysis
```{r}

# using https://www.kaggle.com/datasets/metmuseum/the-metropolitan-museum-of-art-open-access

df <- read.csv("~/Downloads/MetObjects.csv")
#summary(df)

#colSums(is.na(df)) # this dataset has been previously cleaned, no missing values 

highlightT <- df[df$Is.Highlight == TRUE,]

```
      

```{r}
library(ggplot2)
library(plotly)
#table(df$Department)

p1 <- ggplot(df, aes(x = Department)) + 
  geom_bar() + theme_light() + theme(axis.text.x = element_text(angle = 45, size = 5, hjust=1))
ggplotly(p1)

```
      
Most of the artworks at the Met at Drawings and Prints. This makes sense becuase these are probably the easiest high medium to make! Photographs are also low effort to make and thus they are in high quantity. 

```{r}
p2 <- ggplot(df, aes(x = Is.Highlight)) + 
  geom_bar() + theme_light() + ggtitle(label = "Is the artwork a highlight?")

ggplotly(p2)

```

```{r}
p3 <- ggplot(df, aes(x = Is.Public.Domain)) + 
  geom_bar() + theme_light() + ggtitle(label = "Is the artwork in public domain?")

ggplotly(p3)
```

```{r}
library(OneR)

set.seed(5293)
n <- nrow(df)
train <- sample(n, .8*n)
train_dat <- df[train, ]
test_dat <- df[-train, ]

mod <- OneR(Is.Highlight ~ Classification + Country + Artist.Nationality + Artist.Display.Name + Object.Name + Department + Is.Public.Domain + Object.Date, data = train_dat)
#summary(mod)

#mean(predict(mod, train_dat) == train_dat$Is.Highlight)

#mean(predict(mod, test_dat) == test_dat$Is.Highlight)



```
      
The One Rule Model does not work for this task. The summary of the model is incredibly dense, and the "One Rule" chosen is in fact the name of the artist. For a datset of this size, it is almost impossible to interpret the model results because the model results are no better than me walking into the Met with a list of famous artists in my head. Van Gogh = Is.Highlight I would expect, and that is the case. 

## The Model

```{r}

# sampling 1000 rows 

set.seed(5293)

data_s1 <- df[sample(1:nrow(df), 1000), ]
#head(data_s1)   

n <- nrow(data_s1)
train <- sample(n, .8*n)
train_dat <- data_s1[train, ]
test_dat <- data_s1[-train, ]

mod <- OneR(Is.Highlight ~ Classification + Country + Artist.Nationality + Artist.Display.Name + Object.Name + Department + Is.Public.Domain + Object.Date, data = train_dat)
#summary(mod)

mean(predict(mod, train_dat) == train_dat$Is.Highlight)

mean(predict(mod, test_dat) == test_dat$Is.Highlight)

```

      
With the oneRule model, we are able to find out that the Object.Date column is able to predict the training dataset 96% of the time; however, on the test dataset it is only able to predict 46.5% of the time. This means that the oneRule model is not very accurate, and thus we should try more models. 
      
It also tells us that 46.5% of the variation in the IsHighlight variable can be explained with the Object.Date variable in this sample. Let's see if that holds up if we use a larger sample!

```{r}

# sampling 1000 rows 

set.seed(5293)

data_s2 <- df[sample(1:nrow(df), 100000), ]
#head(data_s2)   
write.csv(data_s2, "met_data_sample_100k")

n <- nrow(data_s1)
train <- sample(n, .8*n)
train_dat <- data_s1[train, ]
test_dat <- data_s1[-train, ]

mod <- OneR(Is.Highlight ~ Classification + Country + Artist.Nationality + Artist.Display.Name + Object.Name + Department + Is.Public.Domain + Object.Date, data = train_dat)
#summary(mod)

mean(predict(mod, train_dat) == train_dat$Is.Highlight)

mean(predict(mod, test_dat) == test_dat$Is.Highlight)
```

With a dataset of 100,000 randomly selected rows, the model chooses to use the Object.Date as the explanatory variable.  
      
For this larger sample, we find that it is  able to predict the class from the object data with 96% accuracy on the training set, while it is able to predict the isHighlight variable with a 47% accuracy. 
      
While this project focuses on interpretability, it is clear that the model does not capture the nuances of the isHighlight variable and thus it is hard to trust the results. Thus, we will move onto the next option. 


```{r}
library(ggplot2)
library(waffle)

department <- head(sort(table(df$Department), decreasing = TRUE), 8)
waffle(department/1000, rows=10, size=0.6, 
       title="Art Department", 
       xlab="1 square = 1000 artworks")

period <- head(sort(table(df$Period), decreasing = TRUE), 8)
waffle(period/1000, rows=5, size=0.6, 
       title="Art Period", 
       xlab="1 square = 1000 artworks")
```
      

The period and culture variables are dominated by empty values. The department is not - something that the Met must use as a catch-all term for all different types of artworks. To fix the empty values, we will engage in basic data cleaning. 

      
Basic Data Cleaning:       
- all to lowercase      
- removed punctuation, "possibly" and "probably"       
- Removed columns with significant NA values: Period, Dynasty, Reign, Portfolio, Artist Prefix, Excavation, River, Repository, etc.       
      

```{r}
library(dplyr)
library(janitor)

#df <- read.csv("~/Downloads/MetObjects.csv")

#names(which(colSums(is.na(df)) > 0))

#df <- df[-which(df$Culture == ""),]
#df <- df[-which(df$Country == ""),]

#df1 <- df %>% select(!c(Link.Resource, Repository, Rights.and.Reproduction, Object.Number, Credit.Line, Locus,  
#                        Period, Dynasty, Reign, Portfolio,
#                        Artist.Prefix, Artist.Suffix,
#                        Locale, Excavation, River,
#                        Metadata.Date)) 

#df1 <- df1 %>%
#  rename_all(tolower) %>%
#  mutate(across(everything(), ~gsub("[^[:alnum:]]", "_", .x))) %>%
#  mutate(across(everything(), ~gsub("[[:punct:]]", "", .x))) %>%
#  mutate(across(everything(), ~gsub("&", "and", .x))) %>%
#  mutate(across(everything(), ~gsub("probably", "", .x))) %>%# for all my intents and purposes the "probably" is good enough 
#  mutate(across(everything(), ~gsub("possibly", "", .x))) # for all my intents and purposes the "probably" is good enough 

# cleaner dataset! 

#write.csv(df1, "~/Downloads/MetObjects_Cleaned.csv")

```



```{r}
#df <- read.csv("~/Downloads/MetObjects_Cleaned.csv")

#explore_true <- df[which(df$is.highlight == "True"),]
#explore_false <- df[which(df$is.highlight == "False"),]
#df_small <- explore_false[sample(1:nrow(explore_false), 1859), ]

#df2 <- rbind(explore_true, df_small)

#write.csv(df2, "~/Downloads/MetObjects_Cleaned_Small.csv")

```

## Model 1 
All variables

```{r}
library(OneR)

df <- read.csv("~/Downloads/MetObjects_Cleaned_Small.csv")

#Classification + Country + Artist.Nationality + Artist.Display.Name + Object.Name + Department + Is.Public.Domain + Object.Date + Artist.Role + Geography.Type + Region + Medium + Portfolio + Object.Begin.Date + Object.End.Date + Artist.Begin.Date + Artist.End.Date + Artist.Nationality
n <- nrow(df)
train <- sample(n, .8*n)
train_dat <- df[train, ]
test_dat <- df[-train, ]

mod2 <- OneR(is.highlight ~ ., data = train_dat)
#summary(mod2)
#plot(mod2)

print(paste0("The accuracy on the training dataset is: ",mean(predict(mod2, train_dat) == train_dat$is.highlight)))

print(paste0("The accuracy on the testing dataset is: ",mean(predict(mod2, test_dat) == test_dat$is.highlight)))

```

In this model, the One Rule is the title of the artwork, chosen from all of the possible variables in the dataset. Using the art title, we perform only a little better than random on the dataset and we perform much worse than random on the test set. This is a great example of overfitting, because the model performs significantly better on the training set than on the test set. 

## Model 2 
Only Public Domain 

```{r}
mod2 <- OneR(is.highlight ~ is.public.domain, data = train_dat)
summary(mod2)
plot(mod2)

print(paste0("The accuracy on the training dataset is: ",mean(predict(mod2, train_dat) == train_dat$is.highlight)))

print(paste0("The accuracy on the testing dataset is: ",mean(predict(mod2, test_dat) == test_dat$is.highlight)))
```
      
This model is based only on the is.public.domain variable. From this, we can tell that a large majority of artworks in the public domain are highlights (66%), compared to the large amounts of works that are not in the public domain and are not highlights (~75%). 
     
As a baseline, this means that with only one variable "is.public.domain", we can achieve an accuracy of .68 on the training and test set. This is a great example where the model has not over or under-fit because they are at the same accuracy. 
     
This baseline will help us understand if the more complicated models are making a large improvement on the data or not. 
    