# Support Vector Machine 
      
In order to do the next two models, the data needed to be processed differently to account for the categories that appear in the training set and not the test set, or vice versa.        
            
      
```{r}
library(e1071)
library(dplyr)
library(caret)
str = "Ábcdêãçoàúü"
str = iconv(str, from = '', to = 'ASCII//TRANSLIT')
#gsub("[[:punct:]]", "", str)

remove_exotic <- function(str){
  str = iconv(str, from = '', to = 'ASCII//TRANSLIT')
  return(gsub("[[:punct:]]", "", str))
}
```

## The Model

```{r include = FALSE, eval = FALSE}

df <- read.csv("~/Downloads/MetObjects_Cleaned_Small.csv")

n <- nrow(df)

train <- sample(n, .8*n)
train_dat <- df[train, ]
test_dat <- df[-train, ]

train_dat2 <- train_dat %>% select(c(is.highlight, is.public.domain, department, country, region, subregion,state, city, classification)) 
test_dat2 <- test_dat %>% select(c(is.highlight, is.public.domain, department, country, region, subregion,state, city, classification)) 

train_dat2$is.highlight <- as.factor(train_dat2$is.highlight)
test_dat2$is.highlight <- as.factor(test_dat2$is.highlight)


td3 <- na.omit(train_dat2)
testd3 <- na.omit(test_dat2)

td3$city <- sapply(td3$city,remove_exotic)
testd3$city <- sapply(testd3$city,remove_exotic)
td3$state <- sapply(td3$state,remove_exotic)
testd3$state <- sapply(testd3$state,remove_exotic)
td3$region <- sapply(td3$region,remove_exotic)
testd3$region <- sapply(testd3$region,remove_exotic)
td3$subregion <- sapply(td3$subregion,remove_exotic)
testd3$subregion <- sapply(testd3$subregion,remove_exotic)

td3 <- as.data.frame(td3)
testd3 <- as.data.frame(testd3)

svm_model<- svm(is.highlight~is.public.domain + department + country +
                        region + subregion + state + city +
                        classification, 
                data = td3, 
                type = "C-classification",
                test_datadecision.values=TRUE,
                kernel = "linear")

#pred_train <- predict(svm_model, td3, decision.values = T)
#mean(pred_train == td3$is.highlight)

#pred_test <- predict(svm_model, testd3, decision.values = T)
#mean(pred_test == testd3$is.highlight)

```


```{r include = FALSE, eval = FALSE}
n <- nrow(df)

df$city <- sapply(df$city,remove_exotic)
df$state <- sapply(df$state,remove_exotic)
df$region <- sapply(df$region,remove_exotic)
df$subregion <- sapply(df$subregion,remove_exotic)

#dummy <- dummyVars(" ~ .", data=df)
#newdata <- data.frame(predict(dummy, newdata = df))

#write.csv(newdata, "dummyvars.csv")

```

```{r}
library(mikropml)
library(dplyr)

#df <- read.csv("~/Downloads/MetObjects_Cleaned_Small.csv")

#getting rid of columns with near-zero variance 
#newdata <- read.csv("~/Desktop/dummyvars.csv")
#newdata[,"is.highlightFalse"] <- as.character(newdata[,"is.highlightFalse"])
#dat_proc <- preprocess_data(newdata, 'is.highlightTrue')$dat_transformed

#dat_proc <- dat_proc %>% select(-c(is.highlightFalse_1))
#dat_proc$object.id = df$object.id #adding ID column back into dataset after processing

#n <- nrow(dat_proc)
#train <- sample(n, .8*n)
#train_dat_proc <- dat_proc[train, ]
#test_dat_proc <- dat_proc[-train, ]

#write.csv(train_dat_proc, "train_dat_cleaned2.csv")
#write.csv(test_dat_proc, "test_dat_cleaned2.csv")

train_dat <- read.csv("train_dat_cleaned2.csv")
test_dat <- read.csv("test_dat_cleaned2.csv")
  
total <- rbind(train_dat, test_dat)
n <- nrow(total)
train <- sample(n, .8*n)
train_dat <- total[train, ]
test_dat <- total[-train, ]

svm_model<- svm(is.highlightTrue~. -object.id, 
                data = train_dat, 
                type = "C-classification",
                test_datadecision.values=TRUE,
                kernel = "polynomial", 
                scale = FALSE)

pred_train <- predict(svm_model, train_dat, decision.values = T)
print(paste0("The accuracy on the training dataset is: ",mean(pred_train == train_dat$is.highlightTrue)))

pred_test <- predict(svm_model, test_dat, decision.values = T)
print(paste0("The accuracy on the testing dataset is: ",mean(pred_test == test_dat$is.highlightTrue)))
```
The support vector machine is a major improvement on the accuracy of the model. This model has an accuracy of .94 on the training set and .92 on the test set. This shows that the model is probably not over or underfitting very heavily and has a high accuracy. 

## Visualization

```{r}
library(ggplot2)

train_dat$is.highlight <- as.factor(train_dat$is.highlight) 
scatter_plot <- ggplot(data = train_dat, 
                       aes(x = object.end.date, 
                           y = object.begin.date, color = is.highlight))+   
  geom_point() +   
  scale_color_manual(values = c("red", "blue"))

layered_plot <- 
    scatter_plot + geom_point(data = train_dat[svm_model$index, ], 
                              aes(x = object.end.date, y = object.begin.date), 
                              color = "purple", 
                              size = 4, alpha = 0.5)

#display plot

library(plotly)

ggplotly(layered_plot)

```
      
While SVM is the most accurate model by far, it is very difficult to conduct interpretable ML methods on this model. Even the basic visualizations that are suggested in a lot of tutorials are not easy to do with this dataset, as can be seen in the graph above (you cannot see any information with two binary variables graphs in this manner)! SVM uses hyperplanes and thus visualizations are inherently difficult because it is unclear how one would visualize a multi-dimensional space greater than 3 dimensions. The above graph shows the "decision boundary" that the SVM uses in purple, yet it is difficult to understand what that decision boundary means in two dimensions, because the graph does not appear to truly be split by the line the purple dots create. In simpler terms, the red and blue dots are on both sides.     

     
Through using the SVM, I aim to demonstrate that accuracy, as a metric can often be misleading. It is unclear how good this model is if we are unable to figure out why it is making the decisions it is making. For the purposes of this project, which aims to determine the decision-making process that goes into the selection of the IsHighlight variable, this model is useless. It does not tell us which features contributed to the predictions it made or the the importance of those features. 

```{r}
#newdata <- read.csv("~/Desktop/dummyvars.csv")
#newdata[,"is.highlightFalse"] <- as.character(newdata[,"is.highlightFalse"])
#dat_proc <- preprocess_data(newdata, 'is.highlightTrue')
#dat_proc$grp_feats$grp1 
#dat_proc$grp_feats$grp2 
#dat_proc$grp_feats$grp3
#dat_proc$removed_feats

```
     
Note:       
    
What are grp1, grp2, and grp 3?       
       
The pre-processing package I used collapses perfectly correlated features. These groups are sets of perfectly correlated features. grp1 is the index column and the object number. grp2 is the title of the artist and their name. grp3 is whether the public domain is true and whether it is false. All of these columns are removed and placed into these groups. For all intents and purposes, I deemed these columns un-necessary and removed them. 
     
Reference:       
- https://rpubs.com/cliex159/865583      
- https://web.mit.edu/6.034/wwwbob/svm.pdf       
- https://github.com/SchlossLab/mikRopML/issues/156      
